{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/groda/big_data/blob/master/MapReduce_Primer_HelloWorld.ipynb","timestamp":1714187548609}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["<a href=\"https://github.com/groda/big_data\"><div><img src=\"https://github.com/groda/big_data/blob/master/logo_bdb.png?raw=true\" align=right width=\"90\"></div></a>\n","\n","# MapReduce: A Primer with <code>Hello World!</code>\n","<br>\n","<br>\n","\n","For this tutorial, we are going to download the core Hadoop distribution and run Hadoop in _local standalone mode_:\n","\n","> ❝ _By default, Hadoop is configured to run in a non-distributed mode, as a single Java process._ ❞\n","\n","(see [https://hadoop.apache.org/docs/stable/.../Standalone_Operation](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation))\n","\n","We are going to run a MapReduce job using MapReduce's [streaming application](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Hadoop_Streaming). This is not to be confused with real-time streaming:\n","\n","> ❝ _Hadoop streaming is a utility that comes with the Hadoop distribution. The utility allows you to create and run Map/Reduce jobs with any executable or script as the mapper and/or the reducer._ ❞\n","\n","MapReduce streaming defaults to using [`IdentityMapper`](https://hadoop.apache.org/docs/stable/api/index.html) and [`IdentityReducer`](https://hadoop.apache.org/docs/stable/api/index.html), thus eliminating the need for explicit specification of a mapper or reducer. Finally, we show how to run a map-only job by setting `mapreduce.job.reduce` equal to $0$.\n","\n","Both input and output are standard files since Hadoop's default filesystem is the regular file system, as specified by the `fs.defaultFS` property in [core-default.xml](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/core-default.xml)).\n"],"metadata":{"id":"GzbmlR27wh6e"}},{"cell_type":"markdown","source":["# Download core Hadoop"],"metadata":{"id":"uUbM5R0GwwYw"}},{"cell_type":"code","source":["HADOOP_URL = \"https://dlcdn.apache.org/hadoop/common/stable/hadoop-3.4.0.tar.gz\"\n","\n","import requests\n","import os\n","import tarfile\n","\n","def download_and_extract_targz(url):\n","    response = requests.get(url)\n","    filename = url.rsplit('/', 1)[-1]\n","    HADOOP_HOME = filename[:-7]\n","    # set HADOOP_HOME environment variable\n","    os.environ['HADOOP_HOME'] = HADOOP_HOME\n","    if os.path.isdir(HADOOP_HOME):\n","      print(\"Not downloading, Hadoop folder {} already exists\".format(HADOOP_HOME))\n","      return\n","    if response.status_code == 200:\n","        with open(filename, 'wb') as file:\n","            file.write(response.content)\n","        with tarfile.open(filename, 'r:gz') as tar_ref:\n","            extract_path = tar_ref.extractall(path='.')\n","            # Get the names of all members (files and directories) in the archive\n","            all_members = tar_ref.getnames()\n","            # If there is a top-level directory, get its name\n","            if all_members:\n","              top_level_directory = all_members[0]\n","              print(f\"ZIP file downloaded and extracted successfully. Contents saved at: {top_level_directory}\")\n","    else:\n","        print(f\"Failed to download ZIP file. Status code: {response.status_code}\")\n","\n","\n","download_and_extract_targz(HADOOP_URL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jDgQtQlzw8bL","outputId":"9f930a90-bd06-4211-9800-b4772d75d3c6","executionInfo":{"status":"ok","timestamp":1714187791276,"user_tz":-420,"elapsed":67720,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["ZIP file downloaded and extracted successfully. Contents saved at: hadoop-3.4.0\n"]}]},{"cell_type":"markdown","source":["# Set environment variables"],"metadata":{"id":"3yvb5cw9xEbh"}},{"cell_type":"markdown","source":["## Set `HADOOP_HOME` and `PATH`"],"metadata":{"id":"u6lkrz1dxIiO"}},{"cell_type":"code","source":["# HADOOP_HOME was set earlier when downloading Hadoop distribution\n","print(\"HADOOP_HOME is {}\".format(os.environ['HADOOP_HOME']))\n","\n","os.environ['PATH'] = ':'.join([os.path.join(os.environ['HADOOP_HOME'], 'bin'), os.environ['PATH']])\n","print(\"PATH is {}\".format(os.environ['PATH']))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s7maAwaFxBT_","outputId":"64988606-e015-4e93-f47a-4fd543e911b9","executionInfo":{"status":"ok","timestamp":1714187791276,"user_tz":-420,"elapsed":8,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["HADOOP_HOME is hadoop-3.4.0\n","PATH is hadoop-3.4.0/bin:/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin\n"]}]},{"cell_type":"markdown","source":["## Set `JAVA_HOME`\n","\n","While Java is readily available on Google Colab, we consider the broader scenario of an Ubuntu machine. In this case, we ensure compatibility by installing Java, specifically opting for the `openjdk-19-jre-headless` version."],"metadata":{"id":"4kzJ8cNoxPyK"}},{"cell_type":"code","source":["import shutil\n","\n","# set variable JAVA_HOME (install Java if necessary)\n","def is_java_installed():\n","    os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","    return os.environ['JAVA_HOME']\n","\n","def install_java():\n","    # Uncomment and modify the desired version\n","    # java_version= 'openjdk-11-jre-headless'\n","    # java_version= 'default-jre'\n","    # java_version= 'openjdk-17-jre-headless'\n","    # java_version= 'openjdk-18-jre-headless'\n","    java_version= 'openjdk-19-jre-headless'\n","\n","    print(f\"Java not found. Installing {java_version} ... (this might take a while)\")\n","    try:\n","        cmd = f\"apt install -y {java_version}\"\n","        subprocess_output = subprocess.run(cmd, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n","        stdout_result = subprocess_output.stdout\n","        # Process the results as needed\n","        print(\"Done installing Java {}\".format(java_version))\n","        os.environ['JAVA_HOME'] = os.path.realpath(shutil.which(\"java\")).split('/bin')[0]\n","        print(\"JAVA_HOME is {}\".format(os.environ['JAVA_HOME']))\n","    except subprocess.CalledProcessError as e:\n","        # Handle the error if the command returns a non-zero exit code\n","        print(\"Command failed with return code {}\".format(e.returncode))\n","        print(\"stdout: {}\".format(e.stdout))\n","\n","# Install Java if not available\n","if is_java_installed():\n","    print(\"Java is already installed: {}\".format(os.environ['JAVA_HOME']))\n","else:\n","    print(\"Installing Java\")\n","    install_java()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SauFHVPOxL-Y","outputId":"5fc502ae-40a1-40dc-9f3a-c6f393bf531a","executionInfo":{"status":"ok","timestamp":1714187791276,"user_tz":-420,"elapsed":6,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Java is already installed: /usr/lib/jvm/java-11-openjdk-amd64\n"]}]},{"cell_type":"markdown","source":["# Run a MapReduce job with Hadoop streaming"],"metadata":{"id":"6HFPVX84xbNd"}},{"cell_type":"markdown","source":["## Create a file\n","\n","Write the string\"Hello, World!\" to a local file.<p>**Note:** you will be writing to the file `./hello.txt` in your current directory (denoted by `./`)."],"metadata":{"id":"_yVa55X1xmOb"}},{"cell_type":"code","source":["!echo \"Hello, World!\">./hello.txt"],"metadata":{"id":"9Jz7mJkcxYxw","executionInfo":{"status":"ok","timestamp":1714187791276,"user_tz":-420,"elapsed":4,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Launch the MapReduce \"Hello, World!\" application\n","\n","Since the default filesystem is the local filesystem (as opposed to HDFS) we do not need to upload the local file `hello.txt` to HDFS.\n","\n","Run a MapReduce job with `/bin/cat` as a mapper and no reducer.\n","\n","**Note:** the first step of removing the output directory is necessary because MapReduce does not overwrite data folders by design."],"metadata":{"id":"zSh_Kr5Bxvst"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nb5JryK9xpPA","outputId":"69cf2da3-a4ca-4b7e-d290-69bec75c5bf4","executionInfo":{"status":"ok","timestamp":1714187800168,"user_tz":-420,"elapsed":8895,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["rm: `my_output': No such file or directory\n","2024-04-27 03:16:35,803 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 03:16:36,102 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 03:16:36,103 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 03:16:36,125 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:16:36,514 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 03:16:36,553 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 03:16:37,006 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local367517938_0001\n","2024-04-27 03:16:37,006 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 03:16:37,285 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 03:16:37,287 INFO mapreduce.Job: Running job: job_local367517938_0001\n","2024-04-27 03:16:37,298 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 03:16:37,305 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 03:16:37,314 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:37,314 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:37,378 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 03:16:37,402 INFO mapred.LocalJobRunner: Starting task: attempt_local367517938_0001_m_000000_0\n","2024-04-27 03:16:37,487 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:37,489 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:37,529 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:16:37,553 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 03:16:37,593 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-27 03:16:37,738 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-27 03:16:37,738 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-27 03:16:37,738 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-27 03:16:37,738 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-27 03:16:37,738 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-27 03:16:37,744 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-27 03:16:37,759 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-27 03:16:37,820 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-27 03:16:37,830 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-27 03:16:37,834 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-27 03:16:37,834 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-27 03:16:37,835 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-27 03:16:37,835 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-27 03:16:37,837 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-27 03:16:37,837 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-27 03:16:37,838 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-27 03:16:37,838 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-27 03:16:37,839 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-27 03:16:37,840 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-27 03:16:37,880 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-27 03:16:37,887 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-27 03:16:37,887 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-27 03:16:37,892 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-27 03:16:37,898 INFO mapred.LocalJobRunner: \n","2024-04-27 03:16:37,898 INFO mapred.MapTask: Starting flush of map output\n","2024-04-27 03:16:37,899 INFO mapred.MapTask: Spilling map output\n","2024-04-27 03:16:37,899 INFO mapred.MapTask: bufstart = 0; bufend = 15; bufvoid = 104857600\n","2024-04-27 03:16:37,899 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-27 03:16:37,914 INFO mapred.MapTask: Finished spill 0\n","2024-04-27 03:16:37,957 INFO mapred.Task: Task:attempt_local367517938_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 03:16:37,967 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-27 03:16:37,967 INFO mapred.Task: Task 'attempt_local367517938_0001_m_000000_0' done.\n","2024-04-27 03:16:37,986 INFO mapred.Task: Final Counters for attempt_local367517938_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=854181\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=35\n","\t\tTotal committed heap usage (bytes)=373293056\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-27 03:16:37,987 INFO mapred.LocalJobRunner: Finishing task: attempt_local367517938_0001_m_000000_0\n","2024-04-27 03:16:37,991 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 03:16:37,996 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-27 03:16:38,001 INFO mapred.LocalJobRunner: Starting task: attempt_local367517938_0001_r_000000_0\n","2024-04-27 03:16:38,034 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:38,034 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:38,038 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:16:38,045 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@134834ca\n","2024-04-27 03:16:38,047 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:16:38,111 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-27 03:16:38,141 INFO reduce.EventFetcher: attempt_local367517938_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-27 03:16:38,243 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local367517938_0001_m_000000_0 decomp: 19 len: 23 to MEMORY\n","2024-04-27 03:16:38,264 INFO reduce.InMemoryMapOutput: Read 19 bytes from map-output for attempt_local367517938_0001_m_000000_0\n","2024-04-27 03:16:38,270 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 19, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->19\n","2024-04-27 03:16:38,289 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-27 03:16:38,293 INFO mapreduce.Job: Job job_local367517938_0001 running in uber mode : false\n","2024-04-27 03:16:38,294 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:38,294 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-27 03:16:38,294 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-27 03:16:38,310 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 03:16:38,317 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-27 03:16:38,319 INFO reduce.MergeManagerImpl: Merged 1 segments, 19 bytes to disk to satisfy reduce memory limit\n","2024-04-27 03:16:38,320 INFO reduce.MergeManagerImpl: Merging 1 files, 23 bytes from disk\n","2024-04-27 03:16:38,321 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-27 03:16:38,322 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 03:16:38,323 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 3 bytes\n","2024-04-27 03:16:38,324 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:38,338 INFO mapred.Task: Task:attempt_local367517938_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-27 03:16:38,343 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:38,343 INFO mapred.Task: Task attempt_local367517938_0001_r_000000_0 is allowed to commit now\n","2024-04-27 03:16:38,353 INFO output.FileOutputCommitter: Saved output of task 'attempt_local367517938_0001_r_000000_0' to file:/content/my_output\n","2024-04-27 03:16:38,355 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-27 03:16:38,358 INFO mapred.Task: Task 'attempt_local367517938_0001_r_000000_0' done.\n","2024-04-27 03:16:38,358 INFO mapred.Task: Final Counters for attempt_local367517938_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141992\n","\t\tFILE: Number of bytes written=854231\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=373293056\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 03:16:38,360 INFO mapred.LocalJobRunner: Finishing task: attempt_local367517938_0001_r_000000_0\n","2024-04-27 03:16:38,360 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-27 03:16:39,296 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-27 03:16:39,297 INFO mapreduce.Job: Job job_local367517938_0001 completed successfully\n","2024-04-27 03:16:39,319 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283906\n","\t\tFILE: Number of bytes written=1708412\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=15\n","\t\tMap output materialized bytes=23\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=23\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=35\n","\t\tTotal committed heap usage (bytes)=746586112\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 03:16:39,319 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result\n","\n","If the job executed successfully, an empty file named `_SUCCESS` is expected to be present in the output directory `my_output`.\n","\n","Verify the success of the MapReduce job by checking for the presence of the `_SUCCESS` file."],"metadata":{"id":"OB_fX9u5x55y"}},{"cell_type":"code","source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnvEvYDfx2g4","outputId":"c67dae5c-9f8c-4ed0-d5e2-f6d4fe7b610e","executionInfo":{"status":"ok","timestamp":1714187801952,"user_tz":-420,"elapsed":1787,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}]},{"cell_type":"markdown","source":["**Note:** `hdfs dfs -ls` is the same as `ls` since the default filesystem is the local filesystem."],"metadata":{"id":"BLMnBh44x_YR"}},{"cell_type":"code","source":["!hdfs dfs -ls my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ufAfmGUvx8jW","outputId":"6a34aeef-ccfd-41f6-916f-fee9ae8a77bd","executionInfo":{"status":"ok","timestamp":1714187803845,"user_tz":-420,"elapsed":1896,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2 items\n","-rw-r--r--   1 root root          0 2024-04-27 03:16 my_output/_SUCCESS\n","-rw-r--r--   1 root root         15 2024-04-27 03:16 my_output/part-00000\n"]}]},{"cell_type":"code","source":["!ls -l my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZnKSahPzyCAn","outputId":"dd084148-ee9c-445f-b14a-d7cb26edd993","executionInfo":{"status":"ok","timestamp":1714187804198,"user_tz":-420,"elapsed":356,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["total 4\n","-rw-r--r-- 1 root root 15 Apr 27 03:16 part-00000\n","-rw-r--r-- 1 root root  0 Apr 27 03:16 _SUCCESS\n"]}]},{"cell_type":"markdown","source":["The actual output of the MapReduce job is contained in the file `part-00000` in the output directory."],"metadata":{"id":"v9LmpcaMyG23"}},{"cell_type":"code","source":["!cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eL-Clat5yD8I","outputId":"1ef03347-04b3-49fb-df17-e77bd08791a0","executionInfo":{"status":"ok","timestamp":1714187804198,"user_tz":-420,"elapsed":5,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}]},{"cell_type":"markdown","source":["# MapReduce without specifying mapper or reducer\n","\n","In the previous example, we have seen how to run a MapReduce job without specifying any reducer.\n","\n","Since the only required options for `mapred streaming` are `input` and `output`, we can also run a MapReduce job without specifying a mapper."],"metadata":{"id":"AmpHr_HyyMnM"}},{"cell_type":"code","source":["!mapred streaming -h"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZPWL1AiXyJac","outputId":"2337bafb-a79f-46ec-87c2-4560b083a332","executionInfo":{"status":"ok","timestamp":1714187805853,"user_tz":-420,"elapsed":1658,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["2024-04-27 03:16:45,268 ERROR streaming.StreamJob: Unrecognized option: -h\n","Usage: $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar [options]\n","Options:\n","  -input          <path> DFS input file(s) for the Map step.\n","  -output         <path> DFS output directory for the Reduce step.\n","  -mapper         <cmd|JavaClassName> Optional. Command to be run as mapper.\n","  -combiner       <cmd|JavaClassName> Optional. Command to be run as combiner.\n","  -reducer        <cmd|JavaClassName> Optional. Command to be run as reducer.\n","  -file           <file> Optional. File/dir to be shipped in the Job jar file.\n","                  Deprecated. Use generic option \"-files\" instead.\n","  -inputformat    <TextInputFormat(default)|SequenceFileAsTextInputFormat|JavaClassName>\n","                  Optional. The input format class.\n","  -outputformat   <TextOutputFormat(default)|JavaClassName>\n","                  Optional. The output format class.\n","  -partitioner    <JavaClassName>  Optional. The partitioner class.\n","  -numReduceTasks <num> Optional. Number of reduce tasks.\n","  -inputreader    <spec> Optional. Input recordreader spec.\n","  -cmdenv         <n>=<v> Optional. Pass env.var to streaming commands.\n","  -mapdebug       <cmd> Optional. To run this script when a map task fails.\n","  -reducedebug    <cmd> Optional. To run this script when a reduce task fails.\n","  -io             <identifier> Optional. Format to use for input to and output\n","                  from mapper/reducer commands\n","  -lazyOutput     Optional. Lazily create Output.\n","  -background     Optional. Submit the job and don't wait till it completes.\n","  -verbose        Optional. Print verbose output.\n","  -info           Optional. Print detailed usage.\n","  -help           Optional. Print help message.\n","\n","Generic options supported are:\n","-conf <configuration file>        specify an application configuration file\n","-D <property=value>               define a value for a given property\n","-fs <file:///|hdfs://namenode:port> specify default filesystem URL to use, overrides 'fs.defaultFS' property from configurations.\n","-jt <local|resourcemanager:port>  specify a ResourceManager\n","-files <file1,...>                specify a comma-separated list of files to be copied to the map reduce cluster\n","-libjars <jar1,...>               specify a comma-separated list of jar files to be included in the classpath\n","-archives <archive1,...>          specify a comma-separated list of archives to be unarchived on the compute machines\n","\n","The general command line syntax is:\n","command [genericOptions] [commandOptions]\n","\n","\n","For more details about these options:\n","Use $HADOOP_HOME/bin/hadoop jar hadoop-streaming.jar -info\n","\n","Try -help for more information\n","Streaming Command Failed!\n"]}]},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -input hello.txt \\\n","    -output my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5H2MkIUPyQc2","outputId":"b733dd07-86af-42c4-8aff-c58f882d2b66","executionInfo":{"status":"ok","timestamp":1714187813317,"user_tz":-420,"elapsed":7467,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-27 03:16:47,238 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 03:16:49,557 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 03:16:49,848 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 03:16:49,848 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 03:16:49,894 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:16:50,441 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 03:16:50,492 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 03:16:51,065 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local833381115_0001\n","2024-04-27 03:16:51,066 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 03:16:51,581 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 03:16:51,583 INFO mapreduce.Job: Running job: job_local833381115_0001\n","2024-04-27 03:16:51,610 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 03:16:51,617 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 03:16:51,629 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:51,629 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:51,780 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 03:16:51,799 INFO mapred.LocalJobRunner: Starting task: attempt_local833381115_0001_m_000000_0\n","2024-04-27 03:16:51,928 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:51,933 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:51,979 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:16:51,990 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 03:16:52,008 INFO mapred.MapTask: numReduceTasks: 1\n","2024-04-27 03:16:52,131 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n","2024-04-27 03:16:52,131 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n","2024-04-27 03:16:52,131 INFO mapred.MapTask: soft limit at 83886080\n","2024-04-27 03:16:52,131 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n","2024-04-27 03:16:52,131 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n","2024-04-27 03:16:52,140 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n","2024-04-27 03:16:52,151 INFO mapred.LocalJobRunner: \n","2024-04-27 03:16:52,152 INFO mapred.MapTask: Starting flush of map output\n","2024-04-27 03:16:52,152 INFO mapred.MapTask: Spilling map output\n","2024-04-27 03:16:52,152 INFO mapred.MapTask: bufstart = 0; bufend = 22; bufvoid = 104857600\n","2024-04-27 03:16:52,152 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214396(104857584); length = 1/6553600\n","2024-04-27 03:16:52,161 INFO mapred.MapTask: Finished spill 0\n","2024-04-27 03:16:52,176 INFO mapred.Task: Task:attempt_local833381115_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 03:16:52,179 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-27 03:16:52,180 INFO mapred.Task: Task 'attempt_local833381115_0001_m_000000_0' done.\n","2024-04-27 03:16:52,188 INFO mapred.Task: Final Counters for attempt_local833381115_0001_m_000000_0: Counters: 17\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=852085\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tSpilled Records=1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=337641472\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","2024-04-27 03:16:52,188 INFO mapred.LocalJobRunner: Finishing task: attempt_local833381115_0001_m_000000_0\n","2024-04-27 03:16:52,189 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 03:16:52,194 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n","2024-04-27 03:16:52,204 INFO mapred.LocalJobRunner: Starting task: attempt_local833381115_0001_r_000000_0\n","2024-04-27 03:16:52,224 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:16:52,225 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:16:52,226 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:16:52,230 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@3c4b7298\n","2024-04-27 03:16:52,233 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:16:52,272 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=2382574336, maxSingleShuffleLimit=595643584, mergeThreshold=1572499072, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n","2024-04-27 03:16:52,287 INFO reduce.EventFetcher: attempt_local833381115_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n","2024-04-27 03:16:52,386 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local833381115_0001_m_000000_0 decomp: 26 len: 30 to MEMORY\n","2024-04-27 03:16:52,397 INFO reduce.InMemoryMapOutput: Read 26 bytes from map-output for attempt_local833381115_0001_m_000000_0\n","2024-04-27 03:16:52,406 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 26, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->26\n","2024-04-27 03:16:52,424 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n","2024-04-27 03:16:52,426 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:52,426 INFO reduce.MergeManagerImpl: finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n","2024-04-27 03:16:52,446 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 03:16:52,446 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-27 03:16:52,452 INFO reduce.MergeManagerImpl: Merged 1 segments, 26 bytes to disk to satisfy reduce memory limit\n","2024-04-27 03:16:52,453 INFO reduce.MergeManagerImpl: Merging 1 files, 30 bytes from disk\n","2024-04-27 03:16:52,454 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n","2024-04-27 03:16:52,455 INFO mapred.Merger: Merging 1 sorted segments\n","2024-04-27 03:16:52,465 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 16 bytes\n","2024-04-27 03:16:52,465 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:52,495 INFO mapred.Task: Task:attempt_local833381115_0001_r_000000_0 is done. And is in the process of committing\n","2024-04-27 03:16:52,497 INFO mapred.LocalJobRunner: 1 / 1 copied.\n","2024-04-27 03:16:52,498 INFO mapred.Task: Task attempt_local833381115_0001_r_000000_0 is allowed to commit now\n","2024-04-27 03:16:52,502 INFO output.FileOutputCommitter: Saved output of task 'attempt_local833381115_0001_r_000000_0' to file:/content/my_output\n","2024-04-27 03:16:52,509 INFO mapred.LocalJobRunner: reduce > reduce\n","2024-04-27 03:16:52,509 INFO mapred.Task: Task 'attempt_local833381115_0001_r_000000_0' done.\n","2024-04-27 03:16:52,510 INFO mapred.Task: Final Counters for attempt_local833381115_0001_r_000000_0: Counters: 24\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=142006\n","\t\tFILE: Number of bytes written=852143\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=1\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=337641472\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 03:16:52,511 INFO mapred.LocalJobRunner: Finishing task: attempt_local833381115_0001_r_000000_0\n","2024-04-27 03:16:52,511 INFO mapred.LocalJobRunner: reduce task executor complete.\n","2024-04-27 03:16:52,608 INFO mapreduce.Job: Job job_local833381115_0001 running in uber mode : false\n","2024-04-27 03:16:52,610 INFO mapreduce.Job:  map 100% reduce 100%\n","2024-04-27 03:16:52,611 INFO mapreduce.Job: Job job_local833381115_0001 completed successfully\n","2024-04-27 03:16:52,644 INFO mapreduce.Job: Counters: 30\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=283920\n","\t\tFILE: Number of bytes written=1704228\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tMap output bytes=22\n","\t\tMap output materialized bytes=30\n","\t\tInput split bytes=75\n","\t\tCombine input records=0\n","\t\tCombine output records=0\n","\t\tReduce input groups=1\n","\t\tReduce shuffle bytes=30\n","\t\tReduce input records=1\n","\t\tReduce output records=1\n","\t\tSpilled Records=2\n","\t\tShuffled Maps =1\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=1\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=675282944\n","\tShuffle Errors\n","\t\tBAD_ID=0\n","\t\tCONNECTION=0\n","\t\tIO_ERROR=0\n","\t\tWRONG_LENGTH=0\n","\t\tWRONG_MAP=0\n","\t\tWRONG_REDUCE=0\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 03:16:52,646 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result"],"metadata":{"id":"v7Ks3e96yXuB"}},{"cell_type":"code","source":["%%bash\n","\n","echo \"Check if MapReduce job was successful\"\n","hdfs dfs -test -e my_output/_SUCCESS\n","if [ $? -eq 0 ]; then\n","\techo \"_SUCCESS exists!\"\n","fi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cWAXvG0_yThc","outputId":"e29c2fd1-3d4e-490f-e16e-041fd13adf0f","executionInfo":{"status":"ok","timestamp":1714187815297,"user_tz":-420,"elapsed":1993,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Check if MapReduce job was successful\n","_SUCCESS exists!\n"]}]},{"cell_type":"markdown","source":["Show output"],"metadata":{"id":"t40GgJ2Hya9P"}},{"cell_type":"code","source":["!cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5APWEgoyaRS","outputId":"fb55bf71-62a9-4a99-ac07-cb8604c5f14c","executionInfo":{"status":"ok","timestamp":1714187815297,"user_tz":-420,"elapsed":5,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}]},{"cell_type":"markdown","source":["What happened here is that not having defined any mapper or reducer, the \"Identity\" mapper ([IdentityMapper](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityMapper.html)) and reducer ([IdentityReducer](https://hadoop.apache.org/docs/stable/api/org/apache/hadoop/mapred/lib/IdentityReducer.html)) were used by default (see [Streaming command options](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Streaming_Command_Options))."],"metadata":{"id":"mzfaMVKqyjpC"}},{"cell_type":"markdown","source":["# Run a map-only MapReduce job\n","\n","Not specifying mapper and reducer in the MapReduce job submission does not mean that MapReduce isn't going to run the mapper and reducer steps, it is simply going to use the Identity mapper and reducer.\n","\n","To run a MapReduce job _without_ reducer one needs to use the generic option\n","\n","    \\-D mapreduce.job.reduces=0\n","\n","(see [specifying map-only jobs](https://hadoop.apache.org/docs/stable/hadoop-streaming/HadoopStreaming.html#Specifying_Map-Only_Jobs))."],"metadata":{"id":"lzIuWv7Myndc"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdwKWyVRye27","outputId":"5e53ce24-a692-4996-c142-376ec48ca469","executionInfo":{"status":"ok","timestamp":1714187821803,"user_tz":-420,"elapsed":6509,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-27 03:16:56,647 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 03:16:58,858 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 03:16:59,106 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 03:16:59,106 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 03:16:59,130 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:16:59,429 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 03:16:59,459 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 03:16:59,814 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local2123589440_0001\n","2024-04-27 03:16:59,814 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 03:17:00,114 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 03:17:00,116 INFO mapreduce.Job: Running job: job_local2123589440_0001\n","2024-04-27 03:17:00,126 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 03:17:00,129 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 03:17:00,139 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:17:00,140 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:17:00,210 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 03:17:00,217 INFO mapred.LocalJobRunner: Starting task: attempt_local2123589440_0001_m_000000_0\n","2024-04-27 03:17:00,253 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:17:00,256 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:17:00,287 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:17:00,300 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 03:17:00,318 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-27 03:17:00,361 INFO mapred.LocalJobRunner: \n","2024-04-27 03:17:00,376 INFO mapred.Task: Task:attempt_local2123589440_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 03:17:00,379 INFO mapred.LocalJobRunner: \n","2024-04-27 03:17:00,380 INFO mapred.Task: Task attempt_local2123589440_0001_m_000000_0 is allowed to commit now\n","2024-04-27 03:17:00,390 INFO output.FileOutputCommitter: Saved output of task 'attempt_local2123589440_0001_m_000000_0' to file:/content/my_output\n","2024-04-27 03:17:00,396 INFO mapred.LocalJobRunner: file:/content/hello.txt:0+14\n","2024-04-27 03:17:00,397 INFO mapred.Task: Task 'attempt_local2123589440_0001_m_000000_0' done.\n","2024-04-27 03:17:00,407 INFO mapred.Task: Final Counters for attempt_local2123589440_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855489\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=363855872\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 03:17:00,407 INFO mapred.LocalJobRunner: Finishing task: attempt_local2123589440_0001_m_000000_0\n","2024-04-27 03:17:00,409 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 03:17:01,123 INFO mapreduce.Job: Job job_local2123589440_0001 running in uber mode : false\n","2024-04-27 03:17:01,126 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-27 03:17:01,131 INFO mapreduce.Job: Job job_local2123589440_0001 completed successfully\n","2024-04-27 03:17:01,140 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855489\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=0\n","\t\tTotal committed heap usage (bytes)=363855872\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=28\n","2024-04-27 03:17:01,140 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"markdown","source":["## Verify the result"],"metadata":{"id":"QZIE9yXOyyHJ"}},{"cell_type":"code","source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7Dt3tUI0yu5e","outputId":"d6292275-0dec-480a-ad70-ce897049c207","executionInfo":{"status":"ok","timestamp":1714187824357,"user_tz":-420,"elapsed":2558,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["0\tHello, World!\n"]}]},{"cell_type":"markdown","source":["## Why a map-only application?\n","\n","The advantage of a map-only job is that the sorting and shuffling phases are skipped, so if you do not need that remember to specify `-D mapreduce.job.reduces=0 `.\n","\n","On the other hand, a MapReduce job even with the default `IdentityReducer` will deliver sorted results because the data passed from the mapper to the reducer always gets sorted.\n"],"metadata":{"id":"hUGEUv99y3cM"}},{"cell_type":"markdown","source":["# Improved version of the MapReduce \"Hello, World!\" application\n","\n","Taking into account the previous considerations, here's a more efficient version of the 'Hello, World!' application that bypasses the shuffling and sorting step."],"metadata":{"id":"FhVVFEdKzGcI"}},{"cell_type":"code","source":["%%bash\n","hdfs dfs -rm -r my_output\n","\n","mapred streaming \\\n","    -D mapreduce.job.reduces=0 \\\n","    -input hello.txt \\\n","    -output my_output \\\n","    -mapper '/bin/cat'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jLgMXX2jy0vC","outputId":"bc610832-32eb-40a3-be55-a5a8d262a9b6","executionInfo":{"status":"ok","timestamp":1714187835138,"user_tz":-420,"elapsed":6873,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Deleted my_output\n"]},{"output_type":"stream","name":"stderr","text":["2024-04-27 03:17:10,108 INFO Configuration.deprecation: io.bytes.per.checksum is deprecated. Instead, use dfs.bytes-per-checksum\n","2024-04-27 03:17:12,332 INFO impl.MetricsConfig: Loaded properties from hadoop-metrics2.properties\n","2024-04-27 03:17:12,583 INFO impl.MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).\n","2024-04-27 03:17:12,583 INFO impl.MetricsSystemImpl: JobTracker metrics system started\n","2024-04-27 03:17:12,609 WARN impl.MetricsSystemImpl: JobTracker metrics system already initialized!\n","2024-04-27 03:17:12,883 INFO mapred.FileInputFormat: Total input files to process : 1\n","2024-04-27 03:17:12,907 INFO mapreduce.JobSubmitter: number of splits:1\n","2024-04-27 03:17:13,248 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local409669647_0001\n","2024-04-27 03:17:13,248 INFO mapreduce.JobSubmitter: Executing with tokens: []\n","2024-04-27 03:17:13,545 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n","2024-04-27 03:17:13,547 INFO mapreduce.Job: Running job: job_local409669647_0001\n","2024-04-27 03:17:13,561 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n","2024-04-27 03:17:13,564 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n","2024-04-27 03:17:13,586 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:17:13,586 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:17:13,655 INFO mapred.LocalJobRunner: Waiting for map tasks\n","2024-04-27 03:17:13,666 INFO mapred.LocalJobRunner: Starting task: attempt_local409669647_0001_m_000000_0\n","2024-04-27 03:17:13,706 INFO output.FileOutputCommitter: File Output Committer Algorithm version is 2\n","2024-04-27 03:17:13,709 INFO output.FileOutputCommitter: FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n","2024-04-27 03:17:13,734 INFO mapred.Task:  Using ResourceCalculatorProcessTree : [ ]\n","2024-04-27 03:17:13,745 INFO mapred.MapTask: Processing split: file:/content/hello.txt:0+14\n","2024-04-27 03:17:13,780 INFO mapred.MapTask: numReduceTasks: 0\n","2024-04-27 03:17:13,794 INFO streaming.PipeMapRed: PipeMapRed exec [/bin/cat]\n","2024-04-27 03:17:13,801 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n","2024-04-27 03:17:13,802 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n","2024-04-27 03:17:13,803 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n","2024-04-27 03:17:13,803 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n","2024-04-27 03:17:13,803 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n","2024-04-27 03:17:13,804 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n","2024-04-27 03:17:13,805 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n","2024-04-27 03:17:13,805 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n","2024-04-27 03:17:13,805 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n","2024-04-27 03:17:13,806 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n","2024-04-27 03:17:13,806 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n","2024-04-27 03:17:13,807 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n","2024-04-27 03:17:13,828 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n","2024-04-27 03:17:13,832 INFO streaming.PipeMapRed: Records R/W=1/1\n","2024-04-27 03:17:13,832 INFO streaming.PipeMapRed: MRErrorThread done\n","2024-04-27 03:17:13,833 INFO streaming.PipeMapRed: mapRedFinished\n","2024-04-27 03:17:13,836 INFO mapred.LocalJobRunner: \n","2024-04-27 03:17:13,849 INFO mapred.Task: Task:attempt_local409669647_0001_m_000000_0 is done. And is in the process of committing\n","2024-04-27 03:17:13,851 INFO mapred.LocalJobRunner: \n","2024-04-27 03:17:13,851 INFO mapred.Task: Task attempt_local409669647_0001_m_000000_0 is allowed to commit now\n","2024-04-27 03:17:13,859 INFO output.FileOutputCommitter: Saved output of task 'attempt_local409669647_0001_m_000000_0' to file:/content/my_output\n","2024-04-27 03:17:13,861 INFO mapred.LocalJobRunner: Records R/W=1/1\n","2024-04-27 03:17:13,861 INFO mapred.Task: Task 'attempt_local409669647_0001_m_000000_0' done.\n","2024-04-27 03:17:13,870 INFO mapred.Task: Final Counters for attempt_local409669647_0001_m_000000_0: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=14\n","\t\tTotal committed heap usage (bytes)=396361728\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 03:17:13,871 INFO mapred.LocalJobRunner: Finishing task: attempt_local409669647_0001_m_000000_0\n","2024-04-27 03:17:13,872 INFO mapred.LocalJobRunner: map task executor complete.\n","2024-04-27 03:17:14,558 INFO mapreduce.Job: Job job_local409669647_0001 running in uber mode : false\n","2024-04-27 03:17:14,561 INFO mapreduce.Job:  map 100% reduce 0%\n","2024-04-27 03:17:14,564 INFO mapreduce.Job: Job job_local409669647_0001 completed successfully\n","2024-04-27 03:17:14,572 INFO mapreduce.Job: Counters: 15\n","\tFile System Counters\n","\t\tFILE: Number of bytes read=141914\n","\t\tFILE: Number of bytes written=855001\n","\t\tFILE: Number of read operations=0\n","\t\tFILE: Number of large read operations=0\n","\t\tFILE: Number of write operations=0\n","\tMap-Reduce Framework\n","\t\tMap input records=1\n","\t\tMap output records=1\n","\t\tInput split bytes=75\n","\t\tSpilled Records=0\n","\t\tFailed Shuffles=0\n","\t\tMerged Map outputs=0\n","\t\tGC time elapsed (ms)=14\n","\t\tTotal committed heap usage (bytes)=396361728\n","\tFile Input Format Counters \n","\t\tBytes Read=14\n","\tFile Output Format Counters \n","\t\tBytes Written=27\n","2024-04-27 03:17:14,572 INFO streaming.StreamJob: Output directory: my_output\n"]}]},{"cell_type":"code","source":["!hdfs dfs -test -e my_output/_SUCCESS && cat my_output/part-00000"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sa1UDPr6zKKw","outputId":"9f7a0984-214f-40bd-c1b7-386fc1661ae3","executionInfo":{"status":"ok","timestamp":1714187839021,"user_tz":-420,"elapsed":3885,"user":{"displayName":"long vu","userId":"09832745920720751712"}}},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Hello, World!\t\n"]}]},{"cell_type":"code","source":["print('Long Vu')"],"metadata":{"id":"2MEcGyZLjpuV","executionInfo":{"status":"ok","timestamp":1714187841305,"user_tz":-420,"elapsed":2,"user":{"displayName":"long vu","userId":"09832745920720751712"}},"outputId":"3141d705-4994-4f22-e80f-9c027797c251","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Long Vu\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"XR5qPvRnjqxu"},"execution_count":null,"outputs":[]}]}